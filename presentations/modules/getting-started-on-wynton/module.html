<section>
    <h1>Getting Started on Wynton HPC</h1>
	
</section>

<section>
	<h1>What is Wynton HPC?</h1>
	<ul>
		<li>A computing cluster underlying UCSF's Research Computing Capability</li>
		<li>Includes several hundred compute nodes and a large shared storage system (<a href="https://wynton.ucsf.edu/hpc/about/specs.html">Cluster specifications</a>)</li>
		<li>Funded and administered cooperatively by UCSF campus IT and key research groups</li>
		<li>Free to use for all UCSF researches</li>
		<li><a href="https://wynton.ucsf.edu">https://wynton.ucsf.edu</a></li>
	</ul>
</section>

<section>
    <h1>Getting a Wynton HPC Account</h1>
	
    <ul>
        <li> fill out <a href="https://wynton.ucsf.edu/hpc/about/join.html"> account request form</a></li>
        <li> if you are from Gladstone:
		<ul>
			<li>ask IT for a UID/GID and check the box for "Gladstone" in the form</li>
		</ul>
	<li> after the form is submitted:</li>
	    <ul>
		    <li>the Wynton admins will set up your Wynton HPC account and work with you to make sure you can access the cluster</a></li>    
	</ul>
    </ul>	
</section>

<section>
  <h1>Logging in to Wynton</h1>
  <ul>
	  <li>The primary method to log in to the Wynton HPC cluster is to use an ssh client application</li>
	  <li>If you are using OS X or Linux, there should already be a built-in version of ssh accessible via the terminal</li>
	  <li>On Windows, you might need to install an ssh client application such as <a href="https://www.chiark.greenend.org.uk/~sgtatham/putty/">Putty</a> or <a href="https://mobaxterm.mobatek.net/">MobaXTerm</a></li>
	  <li>The Wynton HPC website should be up to date with information on logging in: <a href="https://wynton.ucsf.edu/hpc/get-started/access-cluster.html">Access Cluster</a></li>
	</ul>
</section>

<section>	
    <h1>ssh login example</h1>

    <pre><code>
{local}$ ssh alice@log2.wynton.ucsf.edu
alice@log2.wynton.ucsf.edu's password:
Last login: Thu Jul 16 17:03:28 2020 
[alice@wynlog2 ~]$
    </code></pre>
	<ul>
		<li>To log in via ssh from the command-line, you would replace "alice" with your Wynton account username</li>
		<li>You can connect to any of the login or data transfer nodes directly</li>
	</ul>
</section>
	
<section>	
    <h1>sftp login example (file transfer)</h1>
	
    <pre><code>
{local}$ sftp alice@log2.wynton.ucsf.edu
alice@log2.wynton.ucsf.edu's password:
Connected to log2.wynton.ucsf.edu.
sftp>
    </code></pre>
	
	<li>To connect via sftp from the command-line, you would replace <kbd>alice</kbd> with your Wynton account username</li>
	<li>You can connect to any of the login or data transfer nodes directly</li>
	<li> <a href="https://github.com/ucsf-wynton/tutorials/wiki/How-to-Move-Files">Wynton tutorials wiki - How to move files</a> </li>
</section>
	
<section>
	<h1>Linux operating system</h1>
	     
    <ul>
        <li>The computing environment's operating system is CentOS 7 Linux</li>
	<li>Using the Linux command line and the bash "shell" are very useful skills to interact with the Wynton HPC environment</li>
        <li>If one is not comfortable using the Linux command line, an introduction is available in the module below</li>   
        <li><a href="https://ucsf-wynton.github.io/tutorials/presentations/modules/unix_shell/index.html">Wynton tutorial - Unix shell (slides)</a></li>
    </ul>  
</section>

<section>
	<h1>Scratch space</h1>
	
    <ul>
        <li> Global scratch</li>
        <li> Local scratch</li>
    </ul>
</section>
    
<section>
	<h1>Global scratch</h1>
	
    <ul>
        <li> mounted as <pre><code>/wynton/scratch</code></pre> </li>
        <li> available as a shared directory from all Wynton nodes</li>
        <li> <i>automatically purged</i> after 2 weeks</li>       
    </ul>
</section>    
    
    
<section>
	<h1>Local scratch</h1>
	
    <ul>
        <li> mounted as <pre><code>/scratch</code></pre> </li>
	<li> each node has its own </li>
        <li> is not shared with other nodes </li>
	<li> <a href="https://wynton.ucsf.edu/hpc/scheduler/using-local-scratch.html">Using local scratch</a> </li>
    </ul>    
	
    
</section>

<section>
	<h1>Types of nodes</h1>
	
    <ul>
      <li> login nodes </li>
      <li> development nodes </li>
      <li> data transfer nodes </li>
      <li> compute nodes </li>
      <li> gpu nodes </li>
    </ul>
</section>

<section>
	<h1>login nodes</h1>
	
    <ul>
        <li> can be logged into directly</li>
        <li> minimal compute resources</li>         
        <li> node names: log1, log2</li>
   </ul>
</section>

<section>
	<h1>development nodes</h1>
	
    <ul>
        <li> cannot be logged into directly </li>
        <li> log in to a login node first</li>     
        <li> node names: dev1, dev2, dev3, gpudev1</li>
    </ul>         
</section>
             
<section>
	<h1>data transfer nodes</h1>
    <ul>
        <li> like login nodes, can be logged into directly</li>
	<li> have 10 Gbps network connections (login nodes have 1 Gpbs)</li>
        <li> node names: dt1, dt2</li>
    </ul>         
</section>
             
<section>
	<h1>compute nodes</h1>
	
    <ul>
        <li> cannot be logged into </li>             
        <li> the scheduler will send jobs to compute nodes </li>     
	<li> for CPU computation </li>    
    </ul>         
             
</section>

<section>
	<h1>gpu nodes</h1>
	
    <ul>
        <li> cannot be logged into </li>
        <li> the schedule will send jobs to gpu nodes </li>
        <li> for GPU computation</li>         
    </ul>         
</section>             
             
<section>
	<h1>Environment modules</h1>
available module repositories (need to be loaded)
<ul>
	<li>CBI : Repository of software shared by the Computational Biology and Informatics (http://cbi.ucsf.edu) at the UCSF Helen Diller Family Comprehensive Cancer Center</li>
	<li>Sali: Repository of software shared by the UCSF Sali Lab</li>
	</ul>
</section>

<section>
	<h1>Loading modules </h1>	
To load a module, use the `module load` command. For example to load R from the CBI module repository 
	
<pre><code>
  module load CBI r
</pre></code>
	
</section>

<section>
	<h1>List available modules </h1>	

	After loading a module

	<pre><code>
$ module avail
</pre></code>
</section>


<section>
	<h1>List modules currently loaded</h1>	

	After loading a module

	<pre><code>
$ module list
</pre></code>
</section>

<section>
	<h1>Unload a module</h1>
	
Use the `module unload` command. For example, do unload the R module if it had been loaded previously	
<pre><code>
$ module unload r 
</pre></code>
</section>

<section>
	<h1>Disable all loaded modules</h1>	

	<pre><code>
$ module purge
</pre></code>
</section>

<section>
	<h1>More on environment modules</h1>

	<a href="https://wynton.ucsf.edu/hpc/software/software-modules.html">Wynton - Software Modules</a>
</section>
	
<section>
	<h1>Other ways of loading software</h1>
    <ul>
	    <li> <a href="https://wynton.ucsf.edu/hpc/software/scl.html">Centos Software Collections (SCL)</a></li>
	    <li> Download a pre-built binary to your own account</li>
	    <li> Build the software in your home directory</li>
        <li> <a href="https://wynton.ucsf.edu/hpc/software/singularity.html">Use a Singularity container</a> (similar to Docker and Docker container images can be converted to Singularity images)</li>
	</ul>
</section>


<section>
	<h1>Submitting jobs</h1>

<ul>
	<li> Wynton uses SGE (Son of Grid Engine) for job scheduling</li>
	<li> example SGE job submission: 
		<pre><code>qsub -l h_rt=00:01:00 -l mem_free=1G my_job.sge</code></pre> </li>
	<li> (replace time, memory and file name with your choices)</li>
<ul>
	<li>	-l h_rt = maximum runtime (hh:mm:ss or seconds)</li>
	<li> -l mem_free = maximum memory (K for kilobytes, M for megabytes, G for gigabytes)</li>
		</ul>		
	</ul>
	
</section>


<section>
	<h1>To check on submitted job</h1>

	<ul>
	<li>Use the qstat command or qstat command along with the job id</li>
	<pre><code>qstat -j 191442</pre></code>	
	</ul>
</section>

<section>
	<h1>After job ran successfully</h1>
	
	<pre><code>grep "usage" my_job_sge.028740</pre></code>
	<ul>
        <li>replace the output file name with actual output file name</li>
	</ul>
		</section>

<section>
	<h1>After a failed job</h1>
	<pre><code>
	tail -100000 /opt/sge/wynton/common/accounting | qacct -f -j 191442
	</pre></code>
	<ul>
		<li> replace 191442 with the actual SGE job id</li>
	</ul>
</section>

<section>
	<h1>How much memory to request</h1>
<ul>
	<li> With experience, trial & error, you can estimate the memory requirements for various types of jobs</li>
	<li> Logs, reports and accountings can help provide clues</li>
	<li> Wynton is relatively forgiving on memory estimates</li>
	<li> If unsure, try 8GB and then increase/decrease accordingly</li>
	</ul>
</section>


<section>
	<h1>More info on submitting jobs</h1>
         <ul>
		 <li><a href="https://github.com/ucsf-wynton/tutorials/wiki/How-to-Submit-Jobs">Wynton tutorials wiki - How to Submit Jobs</a></li> 
	</ul>
</section>




<section>
	<h1>Interactive sessions</h1>
	
    <ul>
        <li> Currently not possible to request interactive jobs via scheduler </li>
        <li> There are dedicated development nodes that can be used short-term interactive development</li>
    </ul>        
</section>

<section>
	<h1>Interactive Python session</h1>
  <ol>
	  <li> ssh to a login node</li>
	  <li> ssh to a dev node</li>
	  <li> type <kbd>python3</kbd> to enter the Python REPL for an interactive session</li>
	  <li> when done, type <kbd>exit()</kbd> to quit session</li>
	</ol> 
	<hr>
	More info on using Python on Wynton at<br>
	<a href="https://wynton.ucsf.edu/hpc/howto/python.html">https://wynton.ucsf.edu/hpc/howto/python.html</a><br>
	
</section>

<section>
	<h1>Interactive R session</h1>
  <ol>
	  <li> ssh to a login node</li>
	  <li> ssh to a dev node</li>
	  <li> type <kbd>R</kbd> to enter the R interactive session</li>
	  <li> when done, type <kbd>q()</kbd> to quit session</li>
	</ol>
	<hr>
	More info on using R on Wynton at<br>
	<a href="https://wynton.ucsf.edu/hpc/howto/r.html">https://wynton.ucsf.edu/hpc/howto/r.html</a>
</section>	

	
<section>
	<h1>Interactive Matlab session</h1>
  <ol>
	  <li> ssh to a login node</li>
	  <li> ssh to a dev node</li>
	  <li> type <kbd>module load matlab</kbd></li>
	  <li> type <kbd>matlab</kbd></li>
  </ol>
	<hr>
	  More info on using Matlab on Wynton at<br>
	  <a href="https://wynton.ucsf.edu/hpc/howto/matlab.html">https://wynton.ucsf.edu/hpc/howto/matlab.html</a>
</section>	

<section>
	<h1>Best practices</h1>

    <ol>
	<li> backup your data if it is important</li>
        <li> login nodes (or dev nodes) to submit batch jobs to the cluster, dev nodes for interactive work</li>
        <li> use local scratch for staging data and computations</li>
        <li> if using conda environments in Anaconda Python, this is best done inside a Singularity container</li>
	<li> X2Go has better performance than X-forwarding</li>	
   </ol>
	
</section>

<section>
	<h1>Troubleshooting</h1>
      Check the job scheduler log files	
    <ul>
	    <li> <i>error log</i>: will be in the directory that the job was launched from and the file name will be formatted as the job script name followed by .e&lt;jobid&gt;
</li>
	    <li> <i>output log</i>: will be in the directory that the job was launched and the file name will be formatted as the job script name followed by .o&lt;jobid&gt;</li>
    </ul>         
</section>

<section>
	<h1>Getting additional help</h1>
	
	<ul>
		<li> Check the <a href="https://wynton.ucsf.edu/hpc">Wynton HPC website</a> </li>
		<li> Email the Wynton HPC system administrators </li>
		<li> Slack </li>
		<li> ServiceNow </li>
		<li> <a href="https://github.com/ucsf-wynton/tutorials/wiki/Getting-Additional-Help">Getting Additional Help - wiki page</a> </li>	    
	</ul>
</section>
	     

