<section>
    <h1>Getting Started on Wynton HPC</h1>
</section>


<section>
    <h1>Getting a Wynton HPC Account</h1>
	
    <ul>
        <li> fill out <a href="https://wynton.ucsf.edu/hpc/about/join.html"> account request form</a></li>
        <li> if you are from Gladstone, ask IT for a UID/GID and check the box for "Gladstone" in the form</li>
	<li> after the form is submitted, the Wynton admins will set up your Wynton HPC account and work with you to make sure you can access the cluster</a></li>    
    </ul>	
</section>

<section>	
    <h1>ssh</h1>

    <pre><code>
ssh &lt;username&gt;@log2.wynton.ucsf.edu
user@log2.wynton.ucsf.edu's password:
Last login: Thu Jul 16 17:03:28 2020 
[wynlog2 ~]$
    </code></pre>
</section>
	
<section>	
    <h1>sftp</h1>
	
    <pre><code>
sftp &lt;username&gt;@log2.wynton.ucsf.edu
user@log2.wynton.ucsf.edu's password:
Connected to log2.wynton.ucsf.edu.
sftp>
    </code></pre>
</section>
	

<section>
	<h1>Scratch space</h1>
	
    <ul>
        <li> Global scratch</li>
        <li> Local scratch</li>
    </ul>
</section>
    
<section>
	<h1>Global scratch</h1>
	
    <ul>
        <li> mounted as <pre><code>/wynton/scratch</code></pre> </li>
        <li> available as a shared directory from all Wynton nodes</li>
        <li> <i>automatically purged</i> after 2 weeks</li>       
    </ul>
</section>    
    
    
<section>
	<h1>Local scratch</h1>
	
    <ul>
        <li> mounted as <pre><code>/scratch</code></pre> </li>
	<li> each node has it's own </li>
        <li> is not shared with other nodes </li>
	<li> <a href="https://wynton.ucsf.edu/hpc/scheduler/using-local-scratch.html">Using local scratch</a> </li>
    </ul>    
	
    
</section>
    
<section>
	<h1>Linux operating system</h1>
	     
    <ul>
        <li> CentOS 7 Linux</li>
        <li> Job scheduler: SGE 8.1.9 (Son of Grid Engine)</li>
	<li> Job scheduler will be transitioning to Slurm</li>
    </ul>  
</section>

<section>
	<h1>Types of nodes</h1>
	
    <ul>
      <li> login nodes </li>
      <li> development nodes </li>
      <li> data transfer nodes </li>
      <li> compute nodes </li>
      <li> gpu nodes </li>
    </ul>
</section>

<section>
	<h1>login nodes</h1>
	
    <ul>
        <li> can be logged into directly</li>
        <li> minimal compute resource</li>         
    </ul>
</section>

<section>
	<h1>development nodes</h1>
	
    <ul>
        <li> cannot be logged into directly</li>
        <li> log in to a login node first</li>     
    </ul>         
</section>
             
<section>
	<h1>data transfer nodes</h1>
    <ul>
        <li> like login nodes, can be logged in to directly</li>
	<li> have 10 Gbps network connections (login nodes have 1 Gpbs)</li>
    </ul>         
</section>
             
<section>
	<h1>compute nodes</h1>
	
    <ul>
        <li> cannot be logged into directly </li>             
        <li> the scheduler will send jobs to compute nodes </li>     
	<li> for CPU computation </li>    
    </ul>         
             
</section>

<section>
	<h1>gpu nodes</h1>
	
    <ul>
        <li> cannot be logged into directly </li>
        <li> the schedule will send jobs to gpu nodes </li>
        <li> for GPU computation</li>         
    </ul>         
</section>             
             
<section>
	<h1>Environment modules</h1>
available module repositories (need to be loaded)
<ul>
	<li>CBI : Repository of software shared by the Computational Biology and Informatics (http://cbi.ucsf.edu) at the UCSF Helen Diller Family Comprehensive Cancer Center</li>
	<li>Sali: Repository of software shared by the UCSF Sali Lab</li>
	</ul>
</section>

<section>
	<h1>Loading modules </h1>	
To load a module, use the `module load` command. For example to load R from the CBI module repository 
	
<pre><code>
  module load CBI r
</pre></code>
	
</section>

<section>
	<h1>List available modules </h1>	

	After loading a module

	<pre><code>
$ module avail
</pre></code>
</section>


<section>
	<h1>List modules currently loaded</h1>	

	After loading a module

	<pre><code>
$ module list
</pre></code>
</section>

<section>
	<h1>Unload a module</h1>
	
Use the `module unload` command. For example, do unload the R module if it had been loaded previously	
<pre><code>
$ module unload r 
</pre></code>
</section>

<section>
	<h1>Disable all loaded modules</h1>	

	<pre><code>
$ module purge
</pre></code>
</section>

<section>
	<h1>More on environment modules</h1>

	<a href="https://wynton.ucsf.edu/hpc/software/software-modules.html">Wynton - Software Modules</a>
</section>
	
<section>
	<h1>Other ways of loading software</h1>
    <ul>
	<li>Centos Software Collections (SCL)</li>
	<li>Build the software in your home directory</li>
        <li>Use a Singularity container (similar to Docker and Docker container images can be converted to Singularity images)</li>
	</ul>
</section>


<section>
	<h1>Submitting a job</h1>
         <ul>
		 <li><a href="https://github.com/ucsf-wynton/tutorials/wiki/How-to-Submit-Jobs">Wynton tutorials wiki - How to Submit Jobs</a></li> 
	</ul>
</section>

<section>
	<h1>Interactive sessions</h1>
	
    <ul>
        <li> Currently not possible to request interactive jobs via scheduler </li>
        <li> There are dedicated development nodes that can be used short-term interactive development</li>
    </ul>        
</section>

<section>
	<h1>Interactive Python session</h1>
  <ol>
	  <li> ssh to a login node</li>
	  <li> ssh to a dev node</li>
	  <li> type `python3` to enter the Python REPL for an interactive session</li>
	  <li> when done, type `exit()` to quit session</li>
	</ol> 
</section>

<section>
	<h1>Interactive R session</h1>
  <ol>
	  <li> ssh to a login node</li>
	  <li> ssh to a dev node</li>
	  <li> type `R` to enter the R interactive session</li>
	  <li> when done, type `q()` to quit session</li>
	</ol>
</section>	

	
<section>
	<h1>Interactive Matlab session</h1>
  <ol>
	  <li> ssh to a login node</li>
	  <li> ssh to a dev node</li>
	  <li> type `module load Sali matlab`</li>
	  <li> type `matlab`</li>
 </ol>
	  
</section>	
	
<section>
	<h1>Troubleshooting</h1>
      Check the job scheduler log files	
    <ul>
	    <li> <i>error log</i>: will be in the directory that the job was launched from and the file name will be formatted as the job script name followed by .e&lt;jobid&gt;
</li>
	    <li> <i>output log</i>: will be in the directory that the job was launched and the file name will be formatted as the job script name followed by .o&lt;jobid&gt;</li>
    </ul>         
</section>

<section>
	<h1>Getting additional help</h1>
	
	<ul>
		<li> Wynton HPC website </li>
		<li> Email the Wynton HPC system administrators </li>
		<li> Slack </li>
		<li> ServiceNow </li>
		<li> Slurm </li>
		<li> <a href="https://github.com/ucsf-wynton/tutorials/wiki/Getting-Additional-Help">Getting Additional Help - wiki page</a> </li>	    
	</ul>
</section>
	     

